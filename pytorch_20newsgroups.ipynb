{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import nltk\n",
    "# import keras\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from string import punctuation, digits\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "18846\n"
     ]
    }
   ],
   "source": [
    "# 读取全集\n",
    "newsgroups_all = fetch_20newsgroups(subset='all').data\n",
    "print(type(newsgroups_all))\n",
    "print(len(newsgroups_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NB_WORDS = 20000 # 保留最常见的20000个单词\n",
    "MAX_SEQUENCE_LENGTH = 1000 # 每篇文档限定1000词以内\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS) # 分词器\n",
    "tokenizer.fit_on_texts(newsgroups_all)\n",
    "\n",
    "# 文本转化为数字序列\n",
    "sequences = tokenizer.texts_to_sequences(newsgroups_all)\n",
    "\n",
    "# 词汇表：单词-数字映射\n",
    "word_index = tokenizer.word_index # {'the': 1, 'to': 2, 'of': 3, 'a': 4, 'and': 5, ……}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 19415, 455, 559, 15, 29, 2552, 1240, 5609, 33, 322, 767, 2175, 2121, 871, 1343, 32, 251, 88, 77, 84, 12087, 455, 559, 15, 7, 122, 228, 63, 3, 2552, 1240, 20, 517, 3490, 50, 1, 1393, 3, 61, 437, 3, 1507, 50, 1, 1302, 2552, 3027, 3, 1, 2701, 309, 7, 122, 243, 16334, 175, 5, 4, 243, 19416, 268, 7, 122, 194, 2, 296, 37, 337, 2, 369, 4389, 22, 4, 243, 3, 7286, 12, 1, 2552, 349, 30, 20, 1502, 137, 2701, 1382, 90, 7, 397, 5987, 74, 2025, 13, 130, 56, 8, 140, 215, 90, 93, 1457, 770, 1963, 56, 8, 97, 4, 308, 9186, 1857, 2, 1306, 6, 1, 2327, 6760, 115, 348, 5987, 21, 4, 308, 3, 1857, 6, 1, 365, 658, 3, 467, 185, 1, 2552, 20, 194, 2, 1985, 1, 66, 3, 3215, 608, 7, 26, 132, 8755, 19, 2, 131, 1, 3280, 2000, 1, 1151, 1457, 770, 283, 2552, 1222], [14, 2333, 1832, 2803, 15, 1285, 129, 29, 67, 315, 847, 2812, 556, 344, 660, 3380, 4868, 12, 2812, 556, 344, 88, 77, 84, 2333, 1832, 2803, 15, 33, 560, 181, 591, 72, 3, 4839, 4815, 603, 219, 426, 5849, 3570, 2812, 32, 383, 38, 2459, 8, 6, 1, 1182, 12, 4, 315, 847, 556, 344, 9, 1572, 2922, 462, 740, 22, 25, 6820, 959, 101, 158, 21, 1493, 983, 16, 2486, 3570, 1142, 462, 740, 5849, 12915, 2636, 367, 3340, 1142, 61, 83, 315, 847, 2812, 344, 178, 322, 24, 360, 1164, 13, 1957, 1285, 129, 17377, 1832, 2803, 15, 125, 7, 7286, 5, 5, 1, 1168, 3, 1465, 123, 546, 56, 101, 8, 149, 5, 41, 93, 1059, 20, 74, 1168, 3, 8276, 129, 95]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179209"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index) # vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照最大文本长度截断文本或补0\n",
    "features = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  283 2552 1222]\n",
      " [   0    0    0 ... 8276  129   95]]\n"
     ]
    }
   ],
   "source": [
    "print(features[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = fetch_20newsgroups(subset='all').target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  3, 17, ...,  3,  1,  7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签 独热编码 ============= pytorch CrossEntropyLoss don't need ont-hot ===============\n",
    "# from keras.utils import to_categorical\n",
    "# labels = to_categorical(labels)\n",
    "# labels[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 1000)\n",
      "(18846,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9045, 1000) (2262, 1000) (7539, 1000)\n",
      "(9045,) (2262,) (7539,)\n"
     ]
    }
   ],
   "source": [
    "# 拆分训练集，验证集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tv, x_test, y_tv, y_test = train_test_split(features, labels, test_size=0.4, shuffle=False) # 测试集占据0.4\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_tv, y_tv, test_size=0.2, shuffle=False) # 验证集占据0.6数据集中的0.2 = 0.12\n",
    "\n",
    "print(x_train.shape, x_val.shape, x_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GloVe pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "# 从GloVe文件中解析出每个词和它所对应的词向量，并用字典的方式存储\n",
    "# 使用的词向量：100维\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as txtfile:\n",
    "    lines = txtfile.readlines()\n",
    "    for line in lines:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_index['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词向量矩阵, 没有的单词的词向量均为0\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, idx in word_index.items():\n",
    "    embed_vector = embeddings_index.get(word)\n",
    "    if embed_vector is not None:\n",
    "        embedding_matrix[idx] = embed_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179210, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分批次训练\n",
    "# def get_batches(x, y, batch_size):  # 会导致 batch_size 不同\n",
    "#     n_batches = len(x) // batch_size\n",
    "\n",
    "#     for ii in range(0, n_batches * batch_size, batch_size):\n",
    "#         # 最后一个批次取余下的所有元素\n",
    "#         if ii != (n_batches - 1) * batch_size: \n",
    "#             every_x, every_y = x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
    "#         else:\n",
    "#             every_x, every_y = x[ii:], y[ii:]\n",
    "#         yield every_x, every_y\n",
    "\n",
    "# iterations \n",
    "def get_batches(x, y, batch_size=100):\n",
    "    # 每个迭代之后最后剩余的s不足一个batch 的 samples 会加入下个epoch的batch\n",
    "    n_batch = len(x) // batch_size\n",
    "    x, y = x[:batch_size*n_batch], y[:batch_size*n_batch]\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        yield x[i:i+batch_size], y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncated backpropagation\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "embed_size = 100\n",
    "hidden_size = 200\n",
    "num_layers = 1\n",
    "num_epochs = 10\n",
    "batch_size = 200\n",
    "# seq_length = 1000\n",
    "# learning_rate = 0.002\n",
    "num_classes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179210"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        # embed word ids to vectors\n",
    "        inputs = self.embed(inputs)\n",
    "        \n",
    "        # forward\n",
    "        out, hidden = self.lstm(inputs, hidden)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        # out = self.sigmoid(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embed): Embedding(179210, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (linear): Linear(in_features=200, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(vocab_size, embed_size, hidden_size, num_layers, num_classes)\n",
    "rnn.cuda()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch nn.Embedding needs to  Long Tensor \n",
    "# x_train = x_train.astype(np.int64)\n",
    "# x_val = x_val.astype(np.int64)\n",
    "# x_test = x_test.astype(np.int64)\n",
    "\n",
    "# x_train.dtype # dtype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.astype(np.int32)\n",
    "# x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### merge 30 epochs #######\n",
    "### ============== tune epochs ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Iterations: 20... Loss: 1.8922\n",
      "Epoch: 1/10... Iterations: 40... Loss: 1.7248\n",
      "Validation Accuracy: 40.409091 %\n",
      "Epoch: 2/10... Iterations: 60... Loss: 1.8462\n",
      "Epoch: 2/10... Iterations: 80... Loss: 1.8866\n",
      "Validation Accuracy: 37.409091 %\n",
      "Epoch: 3/10... Iterations: 100... Loss: 1.7093\n",
      "Epoch: 3/10... Iterations: 120... Loss: 1.7938\n",
      "Validation Accuracy: 42.318182 %\n",
      "Epoch: 4/10... Iterations: 140... Loss: 1.4754\n",
      "Epoch: 4/10... Iterations: 160... Loss: 1.4191\n",
      "Epoch: 4/10... Iterations: 180... Loss: 1.1286\n",
      "Validation Accuracy: 48.090909 %\n",
      "Epoch: 5/10... Iterations: 200... Loss: 1.1340\n",
      "Epoch: 5/10... Iterations: 220... Loss: 1.2323\n",
      "Validation Accuracy: 51.818182 %\n",
      "Epoch: 6/10... Iterations: 240... Loss: 1.1777\n",
      "Epoch: 6/10... Iterations: 260... Loss: 1.1179\n",
      "Validation Accuracy: 57.409091 %\n",
      "Epoch: 7/10... Iterations: 280... Loss: 0.9180\n",
      "Epoch: 7/10... Iterations: 300... Loss: 1.1726\n",
      "Validation Accuracy: 61.590909 %\n",
      "Epoch: 8/10... Iterations: 320... Loss: 0.9154\n",
      "Epoch: 8/10... Iterations: 340... Loss: 0.6861\n",
      "Epoch: 8/10... Iterations: 360... Loss: 0.6388\n",
      "Validation Accuracy: 65.318182 %\n",
      "Epoch: 9/10... Iterations: 380... Loss: 0.6329\n",
      "Epoch: 9/10... Iterations: 400... Loss: 0.6970\n",
      "Validation Accuracy: 65.227273 %\n",
      "Epoch: 10/10... Iterations: 420... Loss: 0.7343\n",
      "Epoch: 10/10... Iterations: 440... Loss: 0.7060\n",
      "Validation Accuracy: 67.818182 %\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "c = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # training\n",
    "    rnn.train()\n",
    "    \n",
    "    # initial hidden state and memory state\n",
    "    states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda(), \n",
    "              Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda())\n",
    "    \n",
    "    for i, (x, y) in enumerate(get_batches(x_train, y_train, batch_size), 1):\n",
    "        inputs = Variable(torch.from_numpy(x).long()).cuda() # trans int tensor to long tensor\n",
    "        targets = Variable(torch.from_numpy(y)).cuda()\n",
    "        \n",
    "        # forward , backward , optim\n",
    "        rnn.zero_grad()\n",
    "        states = detach(states)\n",
    "        outputs, states = rnn(inputs, states)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(rnn.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        c += 1\n",
    "        if c % 20 == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs), \n",
    "                  \"Iterations: {}...\".format(c), \n",
    "                  \"Loss: {:5.4f}\".format(loss.data[0]))\n",
    "    \n",
    "    # evaluate\n",
    "    rnn.eval()\n",
    "\n",
    "    corr = total = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(get_batches(x_val, y_val, batch_size), 1):\n",
    "        inputs = Variable(torch.from_numpy(x).long()).cuda()\n",
    "        # targets = Variable(torch.from_numpy(y))\n",
    "\n",
    "        # forward, backward, optimize\n",
    "        outputs, _ = rnn(inputs, states)\n",
    "        total += y.shape[0]\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "\n",
    "        corr += (pred.data.cpu().numpy() == y).sum()\n",
    "\n",
    "    print('Validation Accuracy: %f %%' % (100 * corr / total))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Iterations: 20... Loss: 0.4513\n",
      "Epoch: 1/10... Iterations: 40... Loss: 0.4644\n",
      "Validation Accuracy: 69.181818 %\n",
      "Epoch: 2/10... Iterations: 60... Loss: 0.5091\n",
      "Epoch: 2/10... Iterations: 80... Loss: 0.5442\n",
      "Validation Accuracy: 71.590909 %\n",
      "Epoch: 3/10... Iterations: 100... Loss: 0.5636\n",
      "Epoch: 3/10... Iterations: 120... Loss: 0.3634\n",
      "Validation Accuracy: 73.000000 %\n",
      "Epoch: 4/10... Iterations: 140... Loss: 0.3175\n",
      "Epoch: 4/10... Iterations: 160... Loss: 0.2846\n",
      "Epoch: 4/10... Iterations: 180... Loss: 0.2258\n",
      "Validation Accuracy: 72.545455 %\n",
      "Epoch: 5/10... Iterations: 200... Loss: 0.1173\n",
      "Epoch: 5/10... Iterations: 220... Loss: 0.2071\n",
      "Validation Accuracy: 75.500000 %\n",
      "Epoch: 6/10... Iterations: 240... Loss: 0.1786\n",
      "Epoch: 6/10... Iterations: 260... Loss: 0.1700\n",
      "Validation Accuracy: 76.272727 %\n",
      "Epoch: 7/10... Iterations: 280... Loss: 0.1853\n",
      "Epoch: 7/10... Iterations: 300... Loss: 0.1815\n",
      "Validation Accuracy: 76.500000 %\n",
      "Epoch: 8/10... Iterations: 320... Loss: 0.1744\n",
      "Epoch: 8/10... Iterations: 340... Loss: 0.1532\n",
      "Epoch: 8/10... Iterations: 360... Loss: 0.1075\n",
      "Validation Accuracy: 77.227273 %\n",
      "Epoch: 9/10... Iterations: 380... Loss: 0.0869\n",
      "Epoch: 9/10... Iterations: 400... Loss: 0.0710\n",
      "Validation Accuracy: 77.181818 %\n",
      "Epoch: 10/10... Iterations: 420... Loss: 0.0767\n",
      "Epoch: 10/10... Iterations: 440... Loss: 0.1290\n",
      "Validation Accuracy: 78.181818 %\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "c = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # training\n",
    "    rnn.train()\n",
    "    \n",
    "    # initial hidden state and memory state\n",
    "    states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda(), \n",
    "              Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda())\n",
    "    \n",
    "    for i, (x, y) in enumerate(get_batches(x_train, y_train, batch_size), 1):\n",
    "        inputs = Variable(torch.from_numpy(x).long()).cuda() # trans int tensor to long tensor\n",
    "        targets = Variable(torch.from_numpy(y)).cuda()\n",
    "        \n",
    "        # forward , backward , optim\n",
    "        rnn.zero_grad()\n",
    "        states = detach(states)\n",
    "        outputs, states = rnn(inputs, states)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(rnn.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        c += 1\n",
    "        if c % 20 == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs), \n",
    "                  \"Iterations: {}...\".format(c), \n",
    "                  \"Loss: {:5.4f}\".format(loss.data[0]))\n",
    "    \n",
    "    # evaluate\n",
    "    rnn.eval()\n",
    "\n",
    "    corr = total = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(get_batches(x_val, y_val, batch_size), 1):\n",
    "        inputs = Variable(torch.from_numpy(x).long()).cuda()\n",
    "        # targets = Variable(torch.from_numpy(y))\n",
    "\n",
    "        # forward, backward, optimize\n",
    "        outputs, _ = rnn(inputs, states)\n",
    "        total += y.shape[0]\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "\n",
    "        corr += (pred.data.cpu().numpy() == y).sum()\n",
    "\n",
    "    print('Validation Accuracy: %f %%' % (100 * corr / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Iterations: 20... Loss: 0.0875\n",
      "Epoch: 1/10... Iterations: 40... Loss: 0.0732\n",
      "Validation Accuracy: 78.000000 %\n",
      "Epoch: 2/10... Iterations: 60... Loss: 0.0602\n",
      "Epoch: 2/10... Iterations: 80... Loss: 0.1877\n",
      "Validation Accuracy: 77.954545 %\n",
      "Epoch: 3/10... Iterations: 100... Loss: 0.0813\n",
      "Epoch: 3/10... Iterations: 120... Loss: 0.0501\n",
      "Validation Accuracy: 78.818182 %\n",
      "Epoch: 4/10... Iterations: 140... Loss: 0.0428\n",
      "Epoch: 4/10... Iterations: 160... Loss: 0.0257\n",
      "Epoch: 4/10... Iterations: 180... Loss: 0.0293\n",
      "Validation Accuracy: 78.863636 %\n",
      "Epoch: 5/10... Iterations: 200... Loss: 0.0282\n",
      "Epoch: 5/10... Iterations: 220... Loss: 0.0302\n",
      "Validation Accuracy: 79.136364 %\n",
      "Epoch: 6/10... Iterations: 240... Loss: 0.0327\n",
      "Epoch: 6/10... Iterations: 260... Loss: 0.0520\n",
      "Validation Accuracy: 80.272727 %\n",
      "Epoch: 7/10... Iterations: 280... Loss: 0.0188\n",
      "Epoch: 7/10... Iterations: 300... Loss: 0.0358\n",
      "Validation Accuracy: 79.318182 %\n",
      "Epoch: 8/10... Iterations: 320... Loss: 0.0220\n",
      "Epoch: 8/10... Iterations: 340... Loss: 0.0272\n",
      "Epoch: 8/10... Iterations: 360... Loss: 0.0065\n",
      "Validation Accuracy: 79.409091 %\n",
      "Epoch: 9/10... Iterations: 380... Loss: 0.0100\n",
      "Epoch: 9/10... Iterations: 400... Loss: 0.0071\n",
      "Validation Accuracy: 81.045455 %\n",
      "Epoch: 10/10... Iterations: 420... Loss: 0.0068\n",
      "Epoch: 10/10... Iterations: 440... Loss: 0.0384\n",
      "Validation Accuracy: 80.954545 %\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "c = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # training\n",
    "    rnn.train()\n",
    "    \n",
    "    # initial hidden state and memory state\n",
    "    states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda(), \n",
    "              Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda())\n",
    "    \n",
    "    for i, (x, y) in enumerate(get_batches(x_train, y_train, batch_size), 1):\n",
    "        inputs = Variable(torch.from_numpy(x).long()).cuda() # trans int tensor to long tensor\n",
    "        targets = Variable(torch.from_numpy(y)).cuda()\n",
    "        \n",
    "        # forward , backward , optim\n",
    "        rnn.zero_grad()\n",
    "        states = detach(states)\n",
    "        outputs, states = rnn(inputs, states)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(rnn.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        c += 1\n",
    "        if c % 20 == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs), \n",
    "                  \"Iterations: {}...\".format(c), \n",
    "                  \"Loss: {:5.4f}\".format(loss.data[0]))\n",
    "    \n",
    "    # evaluate\n",
    "    rnn.eval()\n",
    "\n",
    "    corr = total = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(get_batches(x_val, y_val, batch_size), 1):\n",
    "        inputs = Variable(torch.from_numpy(x).long()).cuda()\n",
    "        # targets = Variable(torch.from_numpy(y))\n",
    "\n",
    "        # forward, backward, optimize\n",
    "        outputs, _ = rnn(inputs, states)\n",
    "        total += y.shape[0]\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "\n",
    "        corr += (pred.data.cpu().numpy() == y).sum()\n",
    "\n",
    "    print('Validation Accuracy: %f %%' % (100 * corr / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 79.675676 %\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "corr = total = 0\n",
    "\n",
    "for i, (x, y) in enumerate(get_batches(x_test, y_test, batch_size), 1):\n",
    "    inputs = Variable(torch.from_numpy(x).long()).cuda()\n",
    "\n",
    "    # forward, backward, optimize\n",
    "    outputs, _ = rnn(inputs, states)\n",
    "    total += targets.size(0)\n",
    "    \n",
    "    _, pred = torch.max(outputs, 1)\n",
    "\n",
    "    corr += (pred.data.cpu().numpy() == y).sum()\n",
    "\n",
    "print('Test Accuracy: %f %%' % (100 * corr / total))\n",
    "# 10 epochs : 0.6632\n",
    "# 30 epochs : 0.7968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RNN - GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "embed_size = 100\n",
    "hidden_size = 200\n",
    "num_layers = 1\n",
    "num_epochs = 30\n",
    "batch_size = 200\n",
    "# seq_length = 1000\n",
    "# learning_rate = 0.002\n",
    "num_classes = 20\n",
    "vocab_size = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRU_RNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        # embed word ids to vectors\n",
    "        inputs = self.embed(inputs)\n",
    "        \n",
    "        # forward\n",
    "        out, hidden = self.gru(inputs, hidden)\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        # out = self.sigmoid(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU_RNN(\n",
      "  (embed): Embedding(179210, 100)\n",
      "  (gru): GRU(100, 200, batch_first=True, dropout=0.2)\n",
      "  (linear): Linear(in_features=200, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "gru_rnn = GRU_RNN(vocab_size, embed_size, hidden_size, num_layers, num_classes)\n",
    "gru_rnn.cuda()\n",
    "print(gru_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gru_rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30... Iterations: 20... Loss: 2.8724\n",
      "Epoch: 1/30... Iterations: 40... Loss: 2.6326\n",
      "Validation Accuracy: 18.68 %\n",
      "Epoch: 2/30... Iterations: 60... Loss: 2.6214\n",
      "Epoch: 2/30... Iterations: 80... Loss: 2.3073\n",
      "Validation Accuracy: 27.55 %\n",
      "Epoch: 3/30... Iterations: 100... Loss: 2.1320\n",
      "Epoch: 3/30... Iterations: 120... Loss: 2.1679\n",
      "Validation Accuracy: 32.59 %\n",
      "Epoch: 4/30... Iterations: 140... Loss: 1.8790\n",
      "Epoch: 4/30... Iterations: 160... Loss: 1.7552\n",
      "Epoch: 4/30... Iterations: 180... Loss: 1.5576\n",
      "Validation Accuracy: 42.32 %\n",
      "Epoch: 5/30... Iterations: 200... Loss: 1.4943\n",
      "Epoch: 5/30... Iterations: 220... Loss: 1.3353\n",
      "Validation Accuracy: 49.73 %\n",
      "Epoch: 6/30... Iterations: 240... Loss: 1.3803\n",
      "Epoch: 6/30... Iterations: 260... Loss: 1.1623\n",
      "Validation Accuracy: 59.50 %\n",
      "Epoch: 7/30... Iterations: 280... Loss: 1.1065\n",
      "Epoch: 7/30... Iterations: 300... Loss: 0.9173\n",
      "Validation Accuracy: 64.45 %\n",
      "Epoch: 8/30... Iterations: 320... Loss: 0.7790\n",
      "Epoch: 8/30... Iterations: 340... Loss: 0.6554\n",
      "Epoch: 8/30... Iterations: 360... Loss: 0.5054\n",
      "Validation Accuracy: 68.86 %\n",
      "Epoch: 9/30... Iterations: 380... Loss: 0.4930\n",
      "Epoch: 9/30... Iterations: 400... Loss: 0.4823\n",
      "Validation Accuracy: 70.64 %\n",
      "Epoch: 10/30... Iterations: 420... Loss: 0.5583\n",
      "Epoch: 10/30... Iterations: 440... Loss: 0.5107\n",
      "Validation Accuracy: 73.41 %\n",
      "Epoch: 11/30... Iterations: 460... Loss: 0.3304\n",
      "Epoch: 11/30... Iterations: 480... Loss: 0.4327\n",
      "Validation Accuracy: 76.27 %\n",
      "Epoch: 12/30... Iterations: 500... Loss: 0.3317\n",
      "Epoch: 12/30... Iterations: 520... Loss: 0.2439\n",
      "Epoch: 12/30... Iterations: 540... Loss: 0.1771\n",
      "Validation Accuracy: 77.59 %\n",
      "Epoch: 13/30... Iterations: 560... Loss: 0.0930\n",
      "Epoch: 13/30... Iterations: 580... Loss: 0.1952\n",
      "Validation Accuracy: 77.82 %\n",
      "Epoch: 14/30... Iterations: 600... Loss: 0.2163\n",
      "Epoch: 14/30... Iterations: 620... Loss: 0.1885\n",
      "Validation Accuracy: 79.64 %\n",
      "Epoch: 15/30... Iterations: 640... Loss: 0.1111\n",
      "Epoch: 15/30... Iterations: 660... Loss: 0.1630\n",
      "Validation Accuracy: 80.95 %\n",
      "Epoch: 16/30... Iterations: 680... Loss: 0.1306\n",
      "Epoch: 16/30... Iterations: 700... Loss: 0.1116\n",
      "Epoch: 16/30... Iterations: 720... Loss: 0.0211\n",
      "Validation Accuracy: 82.41 %\n",
      "Epoch: 17/30... Iterations: 740... Loss: 0.0456\n",
      "Epoch: 17/30... Iterations: 760... Loss: 0.0354\n",
      "Validation Accuracy: 82.14 %\n",
      "Epoch: 18/30... Iterations: 780... Loss: 0.0407\n",
      "Epoch: 18/30... Iterations: 800... Loss: 0.0825\n",
      "Validation Accuracy: 81.77 %\n",
      "Epoch: 19/30... Iterations: 820... Loss: 0.0309\n",
      "Epoch: 19/30... Iterations: 840... Loss: 0.0608\n",
      "Validation Accuracy: 83.00 %\n",
      "Epoch: 20/30... Iterations: 860... Loss: 0.0330\n",
      "Epoch: 20/30... Iterations: 880... Loss: 0.0405\n",
      "Epoch: 20/30... Iterations: 900... Loss: 0.0066\n",
      "Validation Accuracy: 84.14 %\n",
      "Epoch: 21/30... Iterations: 920... Loss: 0.0037\n",
      "Epoch: 21/30... Iterations: 940... Loss: 0.0140\n",
      "Validation Accuracy: 83.50 %\n",
      "Epoch: 22/30... Iterations: 960... Loss: 0.0050\n",
      "Epoch: 22/30... Iterations: 980... Loss: 0.0331\n",
      "Validation Accuracy: 84.14 %\n",
      "Epoch: 23/30... Iterations: 1000... Loss: 0.0090\n",
      "Epoch: 23/30... Iterations: 1020... Loss: 0.0153\n",
      "Validation Accuracy: 84.45 %\n",
      "Epoch: 24/30... Iterations: 1040... Loss: 0.0065\n",
      "Epoch: 24/30... Iterations: 1060... Loss: 0.0157\n",
      "Epoch: 24/30... Iterations: 1080... Loss: 0.0025\n",
      "Validation Accuracy: 84.14 %\n",
      "Epoch: 25/30... Iterations: 1100... Loss: 0.0012\n",
      "Epoch: 25/30... Iterations: 1120... Loss: 0.0082\n",
      "Validation Accuracy: 85.32 %\n",
      "Epoch: 26/30... Iterations: 1140... Loss: 0.0014\n",
      "Epoch: 26/30... Iterations: 1160... Loss: 0.0152\n",
      "Validation Accuracy: 85.09 %\n",
      "Epoch: 27/30... Iterations: 1180... Loss: 0.0011\n",
      "Epoch: 27/30... Iterations: 1200... Loss: 0.0035\n",
      "Validation Accuracy: 85.41 %\n",
      "Epoch: 28/30... Iterations: 1220... Loss: 0.0014\n",
      "Epoch: 28/30... Iterations: 1240... Loss: 0.0010\n",
      "Epoch: 28/30... Iterations: 1260... Loss: 0.0007\n",
      "Validation Accuracy: 85.27 %\n",
      "Epoch: 29/30... Iterations: 1280... Loss: 0.0007\n",
      "Epoch: 29/30... Iterations: 1300... Loss: 0.0010\n",
      "Validation Accuracy: 85.36 %\n",
      "Epoch: 30/30... Iterations: 1320... Loss: 0.0012\n",
      "Epoch: 30/30... Iterations: 1340... Loss: 0.0085\n",
      "Validation Accuracy: 85.05 %\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "c = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # training\n",
    "    gru_rnn.train()\n",
    "    \n",
    "    # initial hidden state and memory state\n",
    "    hidden_state = Variable(torch.zeros(num_layers, batch_size, hidden_size)).cuda()\n",
    "    \n",
    "    for i, (x, y) in enumerate(get_batches(x_train, y_train, batch_size), 1):\n",
    "        inputs = Variable(torch.from_numpy(x).long()).cuda() # trans int tensor to long tensor\n",
    "        targets = Variable(torch.from_numpy(y)).cuda()\n",
    "        \n",
    "        # forward , backward , optim\n",
    "        gru_rnn.zero_grad()\n",
    "        hidden_state = hidden_state.detach()\n",
    "        outputs, hidden_state = gru_rnn(inputs, hidden_state)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(gru_rnn.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        c += 1\n",
    "        if c % 20 == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs), \n",
    "                  \"Iterations: {}...\".format(c), \n",
    "                  \"Loss: {:5.4f}\".format(loss.data[0]))\n",
    "    \n",
    "    # evaluate\n",
    "    gru_rnn.eval()\n",
    "\n",
    "    corr = total = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(get_batches(x_val, y_val, batch_size), 1):\n",
    "        inputs = Variable(torch.from_numpy(x).long()).cuda()\n",
    "\n",
    "        # forward, backward, optimize\n",
    "        outputs, _ = gru_rnn(inputs, hidden_state)\n",
    "        total += y.shape[0]\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "\n",
    "        corr += (pred.data.cpu().numpy() == y).sum()\n",
    "\n",
    "    print('Validation Accuracy: %.2f %%' % (100 * corr / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.364865 %\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "corr = total = 0\n",
    "\n",
    "for i, (x, y) in enumerate(get_batches(x_test, y_test, batch_size), 1):\n",
    "    inputs = Variable(torch.from_numpy(x).long()).cuda()\n",
    "\n",
    "    # forward, backward, optimize\n",
    "    outputs, _ = gru_rnn(inputs, hidden_state)\n",
    "    total += y.shape[0]\n",
    "    _, pred = torch.max(outputs, 1)\n",
    "\n",
    "    corr += (pred.data.cpu().numpy() == y).sum()\n",
    "\n",
    "print('Test Accuracy: %f %%' % (100 * corr / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BiRNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "embed_size = 100\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "# seq_length = 1000\n",
    "# learning_rate = 0.002\n",
    "num_classes = 20\n",
    "\n",
    "vocab_size = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.linear = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        # embed word ids to vectors\n",
    "        inputs = self.embed(inputs)\n",
    "        \n",
    "        # forward\n",
    "        out, hidden = self.lstm(inputs, hidden)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out[:, -1, :])\n",
    "        # out = self.sigmoid(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiRNN(\n",
      "  (embed): Embedding(179210, 100)\n",
      "  (lstm): LSTM(100, 200, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (linear): Linear(in_features=400, out_features=20, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "birnn = BiRNN(vocab_size, embed_size, hidden_size, num_layers, num_classes)\n",
    "birnn.cuda()\n",
    "print(birnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(birnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Iterations: 20... Loss: 2.9681\n",
      "Epoch: 1/10... Iterations: 40... Loss: 2.7714\n",
      "Epoch: 1/10... Iterations: 60... Loss: 2.6161\n",
      "Epoch: 1/10... Iterations: 80... Loss: 2.4141\n",
      "Validation Accuracy: 20.18 %\n",
      "Epoch: 2/10... Iterations: 100... Loss: 2.4139\n",
      "Epoch: 2/10... Iterations: 120... Loss: 2.4123\n",
      "Epoch: 2/10... Iterations: 140... Loss: 2.2170\n",
      "Epoch: 2/10... Iterations: 160... Loss: 2.1818\n",
      "Epoch: 2/10... Iterations: 180... Loss: 2.0224\n",
      "Validation Accuracy: 32.32 %\n",
      "Epoch: 3/10... Iterations: 200... Loss: 2.0026\n",
      "Epoch: 3/10... Iterations: 220... Loss: 1.7042\n",
      "Epoch: 3/10... Iterations: 240... Loss: 2.1644\n",
      "Epoch: 3/10... Iterations: 260... Loss: 1.8117\n",
      "Validation Accuracy: 33.09 %\n",
      "Epoch: 4/10... Iterations: 280... Loss: 1.7456\n",
      "Epoch: 4/10... Iterations: 300... Loss: 2.0381\n",
      "Epoch: 4/10... Iterations: 320... Loss: 1.7734\n",
      "Epoch: 4/10... Iterations: 340... Loss: 1.7862\n",
      "Epoch: 4/10... Iterations: 360... Loss: 1.7141\n",
      "Validation Accuracy: 33.77 %\n",
      "Epoch: 5/10... Iterations: 380... Loss: 1.5361\n",
      "Epoch: 5/10... Iterations: 400... Loss: 1.2988\n",
      "Epoch: 5/10... Iterations: 420... Loss: 1.8564\n",
      "Epoch: 5/10... Iterations: 440... Loss: 1.3551\n",
      "Validation Accuracy: 44.77 %\n",
      "Epoch: 6/10... Iterations: 460... Loss: 1.4301\n",
      "Epoch: 6/10... Iterations: 480... Loss: 1.3953\n",
      "Epoch: 6/10... Iterations: 500... Loss: 1.5117\n",
      "Epoch: 6/10... Iterations: 520... Loss: 1.2052\n",
      "Epoch: 6/10... Iterations: 540... Loss: 1.0400\n",
      "Validation Accuracy: 51.68 %\n",
      "Epoch: 7/10... Iterations: 560... Loss: 0.9956\n",
      "Epoch: 7/10... Iterations: 580... Loss: 0.8462\n",
      "Epoch: 7/10... Iterations: 600... Loss: 1.2682\n",
      "Epoch: 7/10... Iterations: 620... Loss: 1.1423\n",
      "Validation Accuracy: 54.23 %\n",
      "Epoch: 8/10... Iterations: 640... Loss: 0.8629\n",
      "Epoch: 8/10... Iterations: 660... Loss: 1.0367\n",
      "Epoch: 8/10... Iterations: 680... Loss: 1.0481\n",
      "Epoch: 8/10... Iterations: 700... Loss: 0.9474\n",
      "Epoch: 8/10... Iterations: 720... Loss: 0.6375\n",
      "Validation Accuracy: 59.05 %\n",
      "Epoch: 9/10... Iterations: 740... Loss: 0.6555\n",
      "Epoch: 9/10... Iterations: 760... Loss: 0.4927\n",
      "Epoch: 9/10... Iterations: 780... Loss: 1.0991\n",
      "Epoch: 9/10... Iterations: 800... Loss: 0.6092\n",
      "Validation Accuracy: 63.05 %\n",
      "Epoch: 10/10... Iterations: 820... Loss: 0.5451\n",
      "Epoch: 10/10... Iterations: 840... Loss: 0.7024\n",
      "Epoch: 10/10... Iterations: 860... Loss: 0.7347\n",
      "Epoch: 10/10... Iterations: 880... Loss: 0.4652\n",
      "Epoch: 10/10... Iterations: 900... Loss: 0.3766\n",
      "Validation Accuracy: 63.59 %\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "c = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    birnn.train()\n",
    "    # initial hidden states and memory states\n",
    "    states = (Variable(torch.zeros(num_layers*2, batch_size, hidden_size)).cuda(), \n",
    "              Variable(torch.zeros(num_layers*2, batch_size, hidden_size)).cuda())\n",
    "    \n",
    "    for i, (x, y) in enumerate(get_batches(x_train, y_train, batch_size), 1):\n",
    "        inputs = Variable(torch.from_numpy(x).long()).cuda()\n",
    "        targets = Variable(torch.from_numpy(y)).cuda()\n",
    "        \n",
    "        # forward backward optimization\n",
    "        optimizer.zero_grad()\n",
    "        states = detach(states)\n",
    "        outputs, states = birnn(inputs, states)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(birnn.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        c += 1\n",
    "        if c % 20 == 0:\n",
    "            print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs), \n",
    "                  \"Iterations: {}...\".format(c), \n",
    "                  \"Loss: {:5.4f}\".format(loss.data[0]))\n",
    "    \n",
    "    # evaluate\n",
    "    birnn.eval()\n",
    "\n",
    "    corr = total = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(get_batches(x_val, y_val, batch_size), 1):\n",
    "        inputs = Variable(torch.from_numpy(x).long()).cuda()\n",
    "\n",
    "        # forward, backward, optimize\n",
    "        outputs, _ = birnn(inputs, states)\n",
    "        total += y.shape[0]\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "\n",
    "        corr += (pred.data.cpu().numpy() == y).sum()\n",
    "\n",
    "    print('Validation Accuracy: %.2f %%' % (100 * corr / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 63.21 %\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "corr = total = 0\n",
    "\n",
    "for i, (x, y) in enumerate(get_batches(x_test, y_test, batch_size), 1):\n",
    "    inputs = Variable(torch.from_numpy(x).long()).cuda()\n",
    "\n",
    "    # forward, backward, optimize\n",
    "    outputs, _ = birnn(inputs, states)\n",
    "    total += y.shape[0]\n",
    "    _, pred = torch.max(outputs, 1)\n",
    "\n",
    "    corr += (pred.data.cpu().numpy() == y).sum() # cpu: runtime error; numpy: type error\n",
    "\n",
    "print('Test Accuracy: %.2f %%' % (100 * corr / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1.])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = np.array([0.1, 0.2, 0.49, 0.52, 0.9])\n",
    "# ta = torch.from_numpy(a)\n",
    "# tb = (torch.round(ta)).cpu().numpy()\n",
    "# tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = np.array([0, 1, 0, 1, 1])\n",
    "# (tb == y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
